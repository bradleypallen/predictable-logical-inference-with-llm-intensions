{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace, BNode\n",
    "from rdflib.namespace import RDF, RDFS, OWL\n",
    "from owlrl import DeductiveClosure, OWLRL_Semantics\n",
    "from intension import Intension\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json, os, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [ \n",
    "    # { \"model_name\": \"gpt-3.5-turbo\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"gpt-4o-2024-05-13\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"gpt-4-0125-preview\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.3\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"claude-3-5-sonnet-20240620\", \"batch_size\": 1 },\n",
    "    { \"model_name\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"claude-3-opus-20240229\", \"batch_size\": 1 },\n",
    "    # { \"model_name\": \"meta-llama/Meta-Llama-3-70B-Instruct\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"claude-3-haiku-20240307\", \"batch_size\": 1 },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRIPLES_SAMPLE_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_blank_node_triple(triple):\n",
    "    s, p, o = triple\n",
    "    return not isinstance(s, BNode) and not isinstance(o, BNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#Truck>, <http://www.w3.org/2000/01/rdf-schema#subClassOf>, <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#MotionCapableThing>\n",
      " 1: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#ProtectiveHeadClothing>, <http://www.w3.org/2000/01/rdf-schema#subClassOf>, <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#ProtectiveHeadClothing>\n",
      " 2: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#feed>, <http://www.w3.org/2000/01/rdf-schema#domain>, <http://www.w3.org/2002/07/owl#Thing>\n",
      " 3: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#HomoPart>, <http://www.w3.org/2002/07/owl#equivalentClass>, <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#HomoPart>\n",
      " 4: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#use>, <http://www.w3.org/2000/01/rdf-schema#domain>, <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#NaturalEnvironmentThing>\n",
      " 5: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#inFrontOf>, <http://www.w3.org/2002/07/owl#equivalentProperty>, <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#inFrontOf>\n",
      " 6: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#Refrigerator>, <http://www.w3.org/2000/01/rdf-schema#subClassOf>, <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#Device>\n",
      " 7: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#Sink>, <http://www.w3.org/2000/01/rdf-schema#subClassOf>, <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#Sink>\n",
      " 8: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#feed>, <http://www.w3.org/2000/01/rdf-schema#domain>, <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#CarryCapableMammal>\n",
      " 9: <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#Hand>, <http://www.w3.org/2000/01/rdf-schema#subClassOf>, <http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#HomoPart>\n"
     ]
    }
   ],
   "source": [
    "# Define vrd: namespace\n",
    "VRD = Namespace(\"http://www.semanticweb.org/nesy4vrd/ontologies/vrd_world#\")\n",
    "\n",
    "# Get Turtle serialization of vrd_world_v1.owl\n",
    "VRD_WORLD_OWL = open(\"data/NeSy4VRD/nesy4vrd_ontology/vrd_world_v1.owl\", \"r\").read()\n",
    "\n",
    "# Create a new graph\n",
    "graph = Graph()\n",
    "graph.bind(\"vrd\", VRD)\n",
    "graph.bind(\"rdf\", RDF)\n",
    "graph.bind(\"rdfs\", RDFS)\n",
    "graph.bind(\"owl\", OWL)\n",
    "graph.parse(\"data/NeSy4VRD/nesy4vrd_ontology/vrd_world_v1.owl\", format='turtle')\n",
    "\n",
    "# Create another graph to store deductive closure\n",
    "closure = Graph()\n",
    "graph.bind(\"vrd\", VRD)\n",
    "closure.bind(\"rdf\", RDF)\n",
    "closure.bind(\"rdfs\", RDFS)\n",
    "closure.bind(\"owl\", OWL)\n",
    "closure += graph\n",
    "DeductiveClosure(OWLRL_Semantics).expand(closure)\n",
    "\n",
    "# Create a graph to store set difference between closure and graph (i.e. the inferred triples)\n",
    "inferred = Graph()\n",
    "graph.bind(\"vrd\", VRD)\n",
    "inferred.bind(\"rdf\", RDF)\n",
    "inferred.bind(\"rdfs\", RDFS)\n",
    "inferred.bind(\"owl\", OWL)\n",
    "inferred += (closure - graph)\n",
    "\n",
    "# Sample from the set of inferred triples, filtering out triples with blank nodes\n",
    "inferred_triples = list(filter(is_not_blank_node_triple, inferred))\n",
    "if TEST_TRIPLES_SAMPLE_SIZE < len(inferred_triples):\n",
    "    inferred_triples = random.sample(inferred_triples, TEST_TRIPLES_SAMPLE_SIZE)\n",
    "\n",
    "# Create a graph to store the sample\n",
    "test = Graph()\n",
    "test.bind(\"vrd\", VRD)\n",
    "test.bind(\"rdf\", RDF)\n",
    "test.bind(\"rdfs\", RDFS)\n",
    "test.bind(\"owl\", OWL)\n",
    "for triple in inferred_triples:\n",
    "    s, p, o = triple\n",
    "    if not isinstance(s, BNode) and not isinstance(o, BNode):\n",
    "        test.add(triple)\n",
    "\n",
    "# Print test triples\n",
    "for i, (s, p, o) in enumerate(test.triples((None, None, None))):\n",
    "    print(f'{i:2d}: <{s}>, <{p}>, <{o}>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [ { \"s\": str(s), \"p\": str(p), \"o\": str(o), \"graph\": VRD_WORLD_OWL } for s, p, o in test.triples((None, None, None)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bradleyallen/Documents/GitHub/predictable-logical-inference-with-llm-intensions/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "gpt-4o-2024-05-13                   : 100%|██████████| 1/1 [00:13<00:00, 13.18s/it]\n",
      "gpt-4-0125-preview                  : 100%|██████████| 1/1 [00:27<00:00, 27.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/bradleyallen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.3  : 100%|██████████| 1/1 [01:09<00:00, 69.55s/it]\n",
      "claude-3-5-sonnet-20240620          : 100%|██████████| 10/10 [01:24<00:00,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/bradleyallen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1: 100%|██████████| 1/1 [00:50<00:00, 50.96s/it]\n",
      "claude-3-opus-20240229              : 100%|██████████| 10/10 [02:58<00:00, 17.85s/it]\n",
      "claude-3-haiku-20240307             : 100%|██████████| 10/10 [00:30<00:00,  3.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODELS:\n",
    "    filename = f'experiments/nesy4vrd/{model[\"model_name\"].split(\"/\")[-1]}-owl-inf.json'\n",
    "    if os.path.isfile(filename):\n",
    "        print(f'{model[\"model_name\"]:36}: EXISTS')\n",
    "    else:\n",
    "        results = []\n",
    "        batches = [ queries[i:i+model[\"batch_size\"]] for i in range(0, len(queries), model[\"batch_size\"]) ] \n",
    "        intension = Intension(model=model[\"model_name\"])\n",
    "        for batch in tqdm(batches, desc=f'{model[\"model_name\"]:36}', total=len(batches)):\n",
    "            response = intension.chain.batch(batch)\n",
    "            for i, result in enumerate(response):\n",
    "                result[\"model\"] = model[\"model_name\"]\n",
    "                result[\"rationale\"] = result[\"text\"][\"rationale\"]\n",
    "                result[\"answer\"] = result[\"text\"][\"answer\"]\n",
    "                result.pop(\"text\")\n",
    "            results.extend(response)\n",
    "        json.dump(results, open(filename, \"w+\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
