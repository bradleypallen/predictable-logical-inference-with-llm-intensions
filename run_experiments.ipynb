{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace, URIRef\n",
    "from rdflib.namespace import RDF, RDFS, OWL\n",
    "from owlrl import DeductiveClosure, OWLRL_Semantics\n",
    "from intension import Intension\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [ \n",
    "    { \"model_name\": \"gpt-3.5-turbo\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"gpt-4o-2024-05-13\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"gpt-4-0125-preview\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.3\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"claude-3-5-sonnet-20240620\", \"batch_size\": 1 },\n",
    "    { \"model_name\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"claude-3-opus-20240229\", \"batch_size\": 1 },\n",
    "    { \"model_name\": \"meta-llama/Meta-Llama-3-70B-Instruct\", \"batch_size\": 50 },\n",
    "    { \"model_name\": \"claude-3-haiku-20240307\", \"batch_size\": 1 },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://example.org/x1>, <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>, <http://example.org/t1>\n",
      "<http://example.org/y1>, <http://www.w3.org/2002/07/owl#sameAs>, <http://example.org/y1>\n",
      "<http://example.org/x1>, <http://www.w3.org/2002/07/owl#sameAs>, <http://example.org/x1>\n",
      "<http://example.org/y2>, <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>, <http://example.org/t2>\n",
      "<http://example.org/y2>, <http://www.w3.org/2002/07/owl#sameAs>, <http://example.org/y2>\n",
      "<http://example.org/y1>, <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>, <http://example.org/t2>\n",
      "<http://example.org/y2>, <http://www.w3.org/2002/07/owl#sameAs>, <http://example.org/y1>\n",
      "<http://example.org/y1>, <http://www.w3.org/2002/07/owl#sameAs>, <http://example.org/y2>\n"
     ]
    }
   ],
   "source": [
    "# Define ex: namespace\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "\n",
    "# Define URIs of individuals of interest\n",
    "individuals = [\n",
    "    URIRef('x1', base=\"http://example.org/\"),\n",
    "    URIRef('y1', base=\"http://example.org/\"),\n",
    "    URIRef('y2', base=\"http://example.org/\")\n",
    "]\n",
    "\n",
    "# Define base knowledge graph in Turtle serialization\n",
    "triples = '''\n",
    "@prefix ex: <http://example.org/> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "\n",
    "ex:x1 ex:r2 ex:y1 .\n",
    "ex:x1 ex:r2 ex:y2 .\n",
    "ex:r2 rdfs:domain ex:t1 .\n",
    "ex:r2 rdfs:range ex:t2 .\n",
    "ex:r2 rdf:type owl:FunctionalProperty .\n",
    "'''\n",
    "\n",
    "# Create a new graph\n",
    "graph = Graph()\n",
    "graph.bind(\"ex\", EX)\n",
    "graph.bind(\"rdf\", RDF)\n",
    "graph.bind(\"rdfs\", RDFS)\n",
    "graph.bind(\"owl\", OWL)\n",
    "graph.parse(data=triples, format='turtle')\n",
    "\n",
    "# Create another graph to store deductive closure\n",
    "closure = Graph()\n",
    "closure.bind(\"ex\", EX)\n",
    "closure.bind(\"rdf\", RDF)\n",
    "closure.bind(\"rdfs\", RDFS)\n",
    "closure.bind(\"owl\", OWL)\n",
    "closure += graph\n",
    "DeductiveClosure(OWLRL_Semantics).expand(closure)\n",
    "\n",
    "# Create a graph to store set difference between closure and graph (i.e. the inferred triples)\n",
    "inferred = Graph()\n",
    "inferred.bind(\"ex\", EX)\n",
    "inferred.bind(\"rdf\", RDF)\n",
    "inferred.bind(\"rdfs\", RDFS)\n",
    "inferred.bind(\"owl\", OWL)\n",
    "inferred += (closure - graph)\n",
    "\n",
    "# Create a graph to store inferred triples with individuals of interest as subjects or objects\n",
    "interesting = Graph()\n",
    "interesting.bind(\"ex\", EX)\n",
    "interesting.bind(\"rdf\", RDF)\n",
    "interesting.bind(\"rdfs\", RDFS)\n",
    "interesting.bind(\"owl\", OWL)\n",
    "for i in individuals:\n",
    "    for s, p, o in inferred.triples((i, None, None)):\n",
    "        interesting.add((s,p,o))\n",
    "    for s, p, o in inferred.triples((None, None, i)):\n",
    "        interesting.add((s,p,o))\n",
    "\n",
    "# Print triples of interest directly\n",
    "for s, p, o in interesting.triples((None, None, None)):\n",
    "    print(f'<{s}>, <{p}>, <{o}>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [ { \"s\": str(s), \"p\": str(p), \"o\": str(o), \"graph\": triples } for s, p, o in interesting.triples((None, None, None)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bradleyallen/Documents/GitHub/predictable-logical-inference-with-llm-intensions/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "gpt-3.5-turbo                       : 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
      "gpt-4o-2024-05-13                   : 100%|██████████| 1/1 [00:10<00:00, 10.62s/it]\n",
      "gpt-4-0125-preview                  : 100%|██████████| 1/1 [00:16<00:00, 16.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/bradleyallen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.3  : 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "claude-3-5-sonnet-20240620          : 100%|██████████| 8/8 [00:45<00:00,  5.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/bradleyallen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1: 100%|██████████| 1/1 [00:14<00:00, 14.44s/it]\n",
      "claude-3-opus-20240229              : 100%|██████████| 8/8 [02:10<00:00, 16.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/bradleyallen/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3-70B-Instruct: 100%|██████████| 1/1 [00:34<00:00, 34.78s/it]\n",
      "claude-3-haiku-20240307             : 100%|██████████| 8/8 [00:23<00:00,  2.97s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODELS:\n",
    "    filename = f'experiments/{model[\"model_name\"].split(\"/\")[-1]}-owl-inf.json'\n",
    "    if os.path.isfile(filename):\n",
    "        print(f'{model[\"model_name\"]:36}: EXISTS')\n",
    "    else:\n",
    "        results = []\n",
    "        batches = [ queries[i:i+model[\"batch_size\"]] for i in range(0, len(queries), model[\"batch_size\"]) ] \n",
    "        intension = Intension(model=model[\"model_name\"])\n",
    "        for batch in tqdm(batches, desc=f'{model[\"model_name\"]:36}', total=len(batches)):\n",
    "            response = intension.chain.batch(batch)\n",
    "            for i, result in enumerate(response):\n",
    "                result[\"model\"] = model[\"model_name\"]\n",
    "                result[\"rationale\"] = result[\"text\"][\"rationale\"]\n",
    "                result[\"predicted\"] = result[\"text\"][\"answer\"]\n",
    "                result.pop(\"text\")\n",
    "            results.extend(response)\n",
    "        json.dump(results, open(filename, \"w+\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
